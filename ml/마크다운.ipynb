{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "\n",
    "_class_ sklearn.tree.DecisionTreeClassifier(_*_,  _criterion='gini'_,  _splitter='best'_,  _max_depth=None_,  _min_samples_split=2_,  _min_samples_leaf=1_,  _min_weight_fraction_leaf=0.0_,  _max_features=None_,  _random_state=None_,  _max_leaf_nodes=None_,  _min_impurity_decrease=0.0_,  _class_weight=None_,  _ccp_alpha=0.0_)\n",
    "\n",
    "A decision tree classifier.\n",
    "\n",
    "\n",
    "\n",
    "Parameters\n",
    "\n",
    "**criterion**{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "분할 품질을 측정하는 함수입니다. 지원되는 기준은 지니 불순물에 대한 \"지니\"와 정보 이득에 대한 \"엔트로피\"이다.\n",
    "\n",
    "**splitter**{“best”, “random”}, default=”best”\n",
    "각 노드에서 분할을 선택하는 데 사용되는 전략입니다. 지원되는 전략은 최적의 분할을 선택하는 \"최적\"과 최적의 무작위 분할을 선택하는 \"무작위\"입니다.\n",
    "\n",
    "**max_depth**int, default=None\n",
    "\n",
    "트리의 최대 깊이입니다. 없음인 경우 모든 잎이 순수해질 때까지 또는 모든 잎에 min_samples_split 샘플이 없을 때까지 노드가 확장됩니다.\n",
    "\n",
    "**min_samples_split**int or float, default=2\n",
    "\n",
    "내부 노드를 분할하는 데 필요한 최소 샘플 수:\n",
    "\n",
    "-int인 경우 'min_samples_split'을 최소 수로 간주합니다.\n",
    "    \n",
    "-   부동의 경우 'min_samples_split'은 분수이고 'ceil(min_samples_split * n_samples)'은 각 분할에 대한 최소 표본 수입니다.\n",
    "    \n",
    "\n",
    "Changed in version 0.18: Added float values for fractions.\n",
    "\n",
    "**min_samples_leaf**int or float, default=1\n",
    "\n",
    "\n",
    "리프 노드에 있어야 하는 최소 샘플 수입니다. 임의의 깊이에서 분할점은 좌우 각 분기에 최소 'min_samples_leaf'의 훈련 샘플을 남겨두는 경우에만 고려된다. 이렇게 하면 특히 회귀 분석에서 모형을 부드럽게 만드는 효과가 있을 수 있습니다.\n",
    "\n",
    "-   int인 경우 'min_samples_leaf'를 최소 수로 간주합니다.\n",
    "    \n",
    "-   부동의 경우 'min_samples_leaf'는 분수이고 'ceil(min_samples_leaf * n_samples)'은 각 노드에 대한 최소 샘플 수입니다.\n",
    "    \n",
    "\n",
    "Changed in version 0.18: Added float values for fractions.\n",
    "\n",
    "**min_weight_fraction_leaf**float, default=0.0\n",
    "\n",
    "리프 노드에 있어야 하는 (모든 입력 샘플의) 가중치 합계의 최소 가중치 부분. sample_weight가 제공되지 않을 경우 표본의 무게가 같습니다.\n",
    "\n",
    "**max_features**int, float or {“auto”, “sqrt”, “log2”}, default=None\n",
    "\n",
    "최적의 분할을 찾을 때 고려해야 할 기능 수:\n",
    "\n",
    "\n",
    "- int인 경우 각 분할에서 'max_features' 기능을 고려하십시오.\n",
    ">     \n",
    "> - 부동인 경우 'max_features'는 분수이고 각 분할에서 'int(max_features * n_features)' 기능이 고려됩니다.\n",
    ">     \n",
    "> - \"auto\"인 경우 'max_sqrt=sqrt(n_sqrt)가 추출됩니다.\n",
    ">     \n",
    "> - \"sqrt\"이면 'max_sqrt=sqrt(n_sqrt)가 추출됩니다.\n",
    ">     \n",
    "> - \"log2\"이면 'max_contract=log2(n_contract)'가 추출됩니다.\n",
    ">     \n",
    "- 없음인 경우 'max_marget=n_marget'이 됩니다.\n",
    ">     \n",
    "\n",
    "참고: \"max_features\" 이상의 기능을 효과적으로 검사해야 하는 경우에도 노드 샘플의 유효한 파티션이 하나 이상 발견될 때까지 분할 검색이 중지되지 않습니다.\n",
    "\n",
    "**random_state**int, RandomState instance or None, default=None\n",
    "\n",
    "추정기의 랜덤성을 제어합니다. splitter가 best로 설정된 경우에도 각 분할에서 형상은 항상 랜덤하게 순열됩니다. \"max_features < n_features\"일 때 알고리즘은 각 분할에서 \"max_features\"를 무작위로 선택한 후 그 중 가장 좋은 분할을 찾는다. 그러나 가장 잘 발견된 분할은 'max_solution=n_solution'이 있더라도 실행마다 다를 수 있습니다. 기준의 개선이 여러 분할에 대해 동일하고 한 분할을 무작위로 선택해야 하는 경우이다. 피팅 중에 결정론적 동작을 얻으려면 'random_state'를 정수로 고정해야 한다.\n",
    "\n",
    "**max_leaf_nodes**int, default=None\n",
    "\n",
    "'max_leaf_nodes'가 있는 나무를 가장 먼저 기른다. 최적 노드는 불순물의 상대적 감소로 정의됩니다. 없음인 경우 리프 노드 수는 무제한입니다.\n",
    "\n",
    "**min_impurity_decrease**float, default=0.0\n",
    "\n",
    "이 분할이 이 값보다 크거나 같은 불순물의 감소를 유도하면 노드가 분할됩니다.\n",
    "\n",
    "\n",
    "**class_weight**dict, list of dict or “balanced”, default=None\n",
    "\n",
    "\n",
    "'{class_label:weight} 형식의 클래스와 연결된 가중치 없는 경우 모든 클래스에 체중 1이 있어야 합니다. 다중 출력 문제의 경우 dict 목록을 y 열과 동일한 순서로 제공할 수 있습니다.\n",
    "\n",
    "다중 출력(멀티라벨 포함)의 경우 자체 딕트에 있는 모든 열의 각 클래스에 대해 가중치를 정의해야 합니다. 예를 들어 4개 클래스 다중 레이블 분류의 경우 가중치는 [{1:1}, {2:5}, {2:5}, {3:1}, {4:1}] 대신 [{0:1, {0:1, 1:5}, {0:1, 1:1}]이어야 합니다.\n",
    "\n",
    "\"균형\" 모드는 y 값을 사용하여 입력 데이터의 클래스 주파수에 반비례하는 가중치를 자동으로 조절합니다. \"n_samples / (n_classes * np.bincount(y))`\n",
    "\n",
    "다중 출력의 경우 각 y 열의 가중치가 곱됩니다.\n",
    "\n",
    "sample_weight가 지정된 경우 이러한 가중치에 sample_weight가 곱됩니다(적합 방법을 통해 전달됨).\n",
    "\n",
    "**ccp_alpha**non-negative float, default=0.0\n",
    "\n",
    "최소 비용-복잡도 제거에 사용되는 복잡도 매개 변수입니다. ccp_alpha보다 작은 비용 복잡도가 가장 큰 하위 트리가 선택됩니다. 기본적으로 가지치기는 수행되지 않습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# GradientBoostingClassifier\n",
    "\n",
    "\n",
    "_class_ sklearn.ensemble.GradientBoostingClassifier(_*_,  _loss='deviance'_,  _learning_rate=0.1_,  _n_estimators=100_,  _subsample=1.0_,  _criterion='friedman_mse'_,  _min_samples_split=2_,  _min_samples_leaf=1_,  _min_weight_fraction_leaf=0.0_,  _max_depth=3_,  _min_impurity_decrease=0.0_,  _init=None_,  _random_state=None_,  _max_features=None_,  _verbose=0_,  _max_leaf_nodes=None_,  _warm_start=False_,  _validation_fraction=0.1_,  _n_iter_no_change=None_,  _tol=0.0001_,  _ccp_alpha=0.0_)\n",
    "\n",
    "Gradient Boosting for classification.\n",
    "\n",
    "GB는 전방 단계별 방식으로 추가 모델을 구축하며, 임의의 미분 가능한 손실 함수의 최적화를 허용한다. 각 단계 'n_classes_'에서 회귀 트리는 이항 또는 다항 이탈도 손실 함수의 음의 기울기에 적합하다. 이항 분류는 단일 회귀 트리가 유도되는 특수한 경우이다.\n",
    "\n",
    "Read more in the  [User Guide](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting).\n",
    "\n",
    "Parameters\n",
    "\n",
    "**loss**{‘deviance’, ‘exponential’}, default=’deviance’\n",
    "\n",
    "최적화할 손실 함수입니다. '일탈'은 확률론적 출력으로 분류하기 위한 이탈(= 로지스틱 회귀)을 나타냅니다. 손실의 경우 '지수' 그레이디언트 부스팅은 AdaBoost 알고리즘을 복구합니다.\n",
    "\n",
    "**learning_rate**float, default=0.1\n",
    "\n",
    "학습률은 각 트리의 기여도를 'learning_rate'만큼 줄인다. learning_rate와 n_estimator 사이에 트레이드오프가 있습니다.\n",
    "\n",
    "**n_estimators**int, default=100\n",
    "\n",
    "수행할 부스팅 단계 수입니다. 그라데이션 부스팅은 과적합에 상당히 강하기 때문에 숫자가 많으면 대개 성능이 향상된다.\n",
    "\n",
    "**subsample**float, default=1.0\n",
    "\n",
    "개별 기본 학습자를 적합시키는 데 사용할 샘플의 비율입니다. 1.0보다 작으면 확률적 그레이디언트 부스팅이 발생한다.  \"parampl\"은 매개 변수 \"n_paramators\"와 상호 작용한다. 서브샘플 < 1.0을 선택하면 분산이 감소하고 편향이 증가합니다.\n",
    "\n",
    "**criterion**{‘friedman_mse’, ‘squared_error’, ‘mse’, ‘mae’}, default=’friedman_mse’\n",
    "\n",
    "분할 품질을 측정하는 함수입니다. 지원되는 기준은 Friedman에 의한 개선 점수를 포함한 평균 제곱 오차의 경우 'friedman_mse', 평균 제곱 오차의 경우 'quared_error', 평균 절대 오차의 경우 'mae'입니다. 'friedman_mse'의 기본값은 어떤 경우에 더 나은 근사치를 제공할 수 있기 때문에 일반적으로 가장 좋습니다.\n",
    "New in version 0.18.\n",
    "\n",
    "\n",
    "**min_samples_split**int or float, default=2\n",
    "\n",
    "\n",
    "내부 노드를 분할하는 데 필요한 최소 샘플 수:\n",
    "\n",
    "-   int인 경우 'min_samples_split'을 최소 수로 간주합니다.\n",
    "    \n",
    "-   부동의 경우 'min_samples_split'은 분수이고 'ceil(min_samples_split * n_samples)'은 각 분할에 대한 최소 표본 수입니다.\n",
    "    \n",
    "\n",
    "Changed in version 0.18: Added float values for fractions.\n",
    "\n",
    "**min_samples_leaf**int or float, default=1\n",
    "\n",
    "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least  `min_samples_leaf`  training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "-   If int, then consider  `min_samples_leaf`  as the minimum number.\n",
    "    \n",
    "-   If float, then  `min_samples_leaf`  is a fraction and  `ceil(min_samples_leaf  *  n_samples)`  are the minimum number of samples for each node.\n",
    "    \n",
    "\n",
    "Changed in version 0.18: Added float values for fractions.\n",
    "\n",
    "**min_weight_fraction_leaf**float, default=0.0\n",
    "\n",
    "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "**max_depth**int, default=3\n",
    "\n",
    "The maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n",
    "\n",
    "**min_impurity_decrease**float, default=0.0\n",
    "\n",
    "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "The weighted impurity decrease equation is the following:\n",
    "\n",
    "N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                    - N_t_L / N_t * left_impurity)\n",
    "\n",
    "where  `N`  is the total number of samples,  `N_t`  is the number of samples at the current node,  `N_t_L`  is the number of samples in the left child, and  `N_t_R`  is the number of samples in the right child.\n",
    "\n",
    "`N`,  `N_t`,  `N_t_R`  and  `N_t_L`  all refer to the weighted sum, if  `sample_weight`  is passed.\n",
    "\n",
    "New in version 0.19.\n",
    "\n",
    "**init**estimator or ‘zero’, default=None\n",
    "\n",
    "An estimator object that is used to compute the initial predictions.  `init`  has to provide  [`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier.fit \"sklearn.ensemble.GradientBoostingClassifier.fit\")  and  [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier.predict_proba \"sklearn.ensemble.GradientBoostingClassifier.predict_proba\"). If ‘zero’, the initial raw predictions are set to zero. By default, a  `DummyEstimator`  predicting the classes priors is used.\n",
    "\n",
    "**random_state**int, RandomState instance or None, default=None\n",
    "\n",
    "Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if  `n_iter_no_change`  is not None. Pass an int for reproducible output across multiple function calls. See  [Glossary](https://scikit-learn.org/stable/glossary.html#term-random_state).\n",
    "\n",
    "**max_features**{‘auto’, ‘sqrt’, ‘log2’}, int or float, default=None\n",
    "\n",
    "The number of features to consider when looking for the best split:\n",
    "\n",
    "-   If int, then consider  `max_features`  features at each split.\n",
    "    \n",
    "-   If float, then  `max_features`  is a fraction and  `int(max_features  *  n_features)`  features are considered at each split.\n",
    "    \n",
    "-   If ‘auto’, then  `max_features=sqrt(n_features)`.\n",
    "    \n",
    "-   If ‘sqrt’, then  `max_features=sqrt(n_features)`.\n",
    "    \n",
    "-   If ‘log2’, then  `max_features=log2(n_features)`.\n",
    "    \n",
    "-   If None, then  `max_features=n_features`.\n",
    "    \n",
    "\n",
    "Choosing  `max_features  <  n_features`  leads to a reduction of variance and an increase in bias.\n",
    "\n",
    "Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than  `max_features`  features.\n",
    "\n",
    "**verbose**int, default=0\n",
    "\n",
    "Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree.\n",
    "\n",
    "**max_leaf_nodes**int, default=None\n",
    "\n",
    "Grow trees with  `max_leaf_nodes`  in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "\n",
    "**warm_start**bool, default=False\n",
    "\n",
    "When set to  `True`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See  [the Glossary](https://scikit-learn.org/stable/glossary.html#term-warm_start).\n",
    "\n",
    "**validation_fraction**float, default=0.1\n",
    "\n",
    "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if  `n_iter_no_change`  is set to an integer.\n",
    "\n",
    "New in version 0.20.\n",
    "\n",
    "**n_iter_no_change**int, default=None\n",
    "\n",
    "`n_iter_no_change`  is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside  `validation_fraction`  size of the training data as validation and terminate training when validation score is not improving in all of the previous  `n_iter_no_change`  numbers of iterations. The split is stratified.\n",
    "\n",
    "New in version 0.20.\n",
    "\n",
    "**tol**float, default=1e-4\n",
    "\n",
    "Tolerance for the early stopping. When the loss is not improving by at least tol for  `n_iter_no_change`  iterations (if set to a number), the training stops.\n",
    "\n",
    "New in version 0.20.\n",
    "\n",
    "**ccp_alpha**non-negative float, default=0.0\n",
    "\n",
    "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than  `ccp_alpha`  will be chosen. By default, no pruning is performed. See  [Minimal Cost-Complexity Pruning](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning)  for details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# KNeighborsClassifier\n",
    "\n",
    "sklearn.neighbors.KNeighborsClassifier(_n_neighbors=5_,  _*_,  _weights='uniform'_,  _algorithm='auto'_,  _leaf_size=30_,  _p=2_,  _metric='minkowski'_,  _metric_params=None_,  _n_jobs=None_)\n",
    "\n",
    "Classifier implementing the k-nearest neighbors vote.\n",
    "\n",
    "Parameters\n",
    "\n",
    "**n_neighbors**int, default=5\n",
    "\n",
    "Number of neighbors to use by default for  [`kneighbors`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=k%20neighbor#sklearn.neighbors.KNeighborsClassifier.kneighbors \"sklearn.neighbors.KNeighborsClassifier.kneighbors\")  queries.\n",
    "\n",
    "**weights**{‘uniform’, ‘distance’} or callable, default=’uniform’\n",
    "\n",
    "Weight function used in prediction. Possible values:\n",
    "\n",
    "-   ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "    \n",
    "-   ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "    \n",
    "-   [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
    "    \n",
    "\n",
    "**algorithm**{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
    "\n",
    "Algorithm used to compute the nearest neighbors:\n",
    "\n",
    "-   ‘ball_tree’ will use  [`BallTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree \"sklearn.neighbors.BallTree\")\n",
    "    \n",
    "-   ‘kd_tree’ will use  [`KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree \"sklearn.neighbors.KDTree\")\n",
    "    \n",
    "-   ‘brute’ will use a brute-force search.\n",
    "    \n",
    "-   ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to  [`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=k%20neighbor#sklearn.neighbors.KNeighborsClassifier.fit \"sklearn.neighbors.KNeighborsClassifier.fit\")  method.\n",
    "    \n",
    "\n",
    "Note: fitting on sparse input will override the setting of this parameter, using brute force.\n",
    "\n",
    "**leaf_size**int, default=30\n",
    "\n",
    "Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n",
    "\n",
    "**p**int, default=2\n",
    "\n",
    "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "\n",
    "**metric**str or callable, default=’minkowski’\n",
    "\n",
    "The distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. For a list of available metrics, see the documentation of  [`DistanceMetric`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html#sklearn.metrics.DistanceMetric \"sklearn.metrics.DistanceMetric\"). If metric is “precomputed”, X is assumed to be a distance matrix and must be square during fit. X may be a  [sparse graph](https://scikit-learn.org/stable/glossary.html#term-sparse-graph), in which case only “nonzero” elements may be considered neighbors.\n",
    "\n",
    "**metric_params**dict, default=None\n",
    "\n",
    "Additional keyword arguments for the metric function.\n",
    "\n",
    "**n_jobs**int, default=None\n",
    "\n",
    "The number of parallel jobs to run for neighbors search.  `None`  means 1 unless in a  [`joblib.parallel_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend \"(in joblib v1.2.0.dev0)\")  context.  `-1`  means using all processors. See  [Glossary](https://scikit-learn.org/stable/glossary.html#term-n_jobs)  for more details. Doesn’t affect  [`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=k%20neighbor#sklearn.neighbors.KNeighborsClassifier.fit \"sklearn.neighbors.KNeighborsClassifier.fit\")  method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# RandomForestClassifier\n",
    "\n",
    "_class_ sklearn.ensemble.RandomForestClassifier(_n_estimators=100_,  _*_,  _criterion='gini'_,  _max_depth=None_,  _min_samples_split=2_,  _min_samples_leaf=1_,  _min_weight_fraction_leaf=0.0_,  _max_features='auto'_,  _max_leaf_nodes=None_,  _min_impurity_decrease=0.0_,  _bootstrap=True_,  _oob_score=False_,  _n_jobs=None_,  _random_state=None_,  _verbose=0_,  _warm_start=False_,  _class_weight=None_,  _ccp_alpha=0.0_,  _max_samples=None_)\n",
    "\n",
    "A random forest classifier.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the  `max_samples`  parameter if  `bootstrap=True`  (default), otherwise the whole dataset is used to build each tree.\n",
    "\n",
    "Parameters\n",
    "\n",
    "**n_estimators**int, default=100\n",
    "\n",
    "The number of trees in the forest.\n",
    "\n",
    "Changed in version 0.22: The default value of  `n_estimators`  changed from 10 to 100 in 0.22.\n",
    "\n",
    "**criterion**{“gini”, “entropy”}, default=”gini”\n",
    "\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.\n",
    "\n",
    "**max_depth**int, default=None\n",
    "\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "**min_samples_split**int or float, default=2\n",
    "\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "-   If int, then consider  `min_samples_split`  as the minimum number.\n",
    "    \n",
    "-   If float, then  `min_samples_split`  is a fraction and  `ceil(min_samples_split  *  n_samples)`  are the minimum number of samples for each split.\n",
    "    \n",
    "\n",
    "Changed in version 0.18: Added float values for fractions.\n",
    "\n",
    "**min_samples_leaf**int or float, default=1\n",
    "\n",
    "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least  `min_samples_leaf`  training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "-   If int, then consider  `min_samples_leaf`  as the minimum number.\n",
    "    \n",
    "-   If float, then  `min_samples_leaf`  is a fraction and  `ceil(min_samples_leaf  *  n_samples)`  are the minimum number of samples for each node.\n",
    "    \n",
    "\n",
    "Changed in version 0.18: Added float values for fractions.\n",
    "\n",
    "**min_weight_fraction_leaf**float, default=0.0\n",
    "\n",
    "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "**max_features**{“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "\n",
    "The number of features to consider when looking for the best split:\n",
    "\n",
    "-   If int, then consider  `max_features`  features at each split.\n",
    "    \n",
    "-   If float, then  `max_features`  is a fraction and  `round(max_features  *  n_features)`  features are considered at each split.\n",
    "    \n",
    "-   If “auto”, then  `max_features=sqrt(n_features)`.\n",
    "    \n",
    "-   If “sqrt”, then  `max_features=sqrt(n_features)`  (same as “auto”).\n",
    "    \n",
    "-   If “log2”, then  `max_features=log2(n_features)`.\n",
    "    \n",
    "-   If None, then  `max_features=n_features`.\n",
    "    \n",
    "\n",
    "Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than  `max_features`  features.\n",
    "\n",
    "**max_leaf_nodes**int, default=None\n",
    "\n",
    "Grow trees with  `max_leaf_nodes`  in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "\n",
    "**min_impurity_decrease**float, default=0.0\n",
    "\n",
    "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "\n",
    "\n",
    "**random_state**int, RandomState instance or None, default=None\n",
    "\n",
    "Controls both the randomness of the bootstrapping of the samples used when building trees (if  `bootstrap=True`) and the sampling of the features to consider when looking for the best split at each node (if  `max_features  <  n_features`). See  [Glossary](https://scikit-learn.org/stable/glossary.html#term-random_state)  for details.\n",
    "\n",
    "**verbose**int, default=0\n",
    "\n",
    "Controls the verbosity when fitting and predicting.\n",
    "\n",
    "**warm_start**bool, default=False\n",
    "\n",
    "When set to  `True`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See  [the Glossary](https://scikit-learn.org/stable/glossary.html#term-warm_start).\n",
    "\n",
    "**class_weight**{“balanced”, “balanced_subsample”}, dict or list of dicts, default=None\n",
    "\n",
    "Weights associated with classes in the form  `{class_label:  weight}`. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
    "\n",
    "Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}].\n",
    "\n",
    "The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as  `n_samples  /  (n_classes  *  np.bincount(y))`\n",
    "\n",
    "The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown.\n",
    "\n",
    "For multi-output, the weights of each column of y will be multiplied.\n",
    "\n",
    "Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.\n",
    "\n",
    "**ccp_alpha**non-negative float, default=0.0\n",
    "\n",
    "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than  `ccp_alpha`  will be chosen. By default, no pruning is performed. See  \n",
    "\n",
    "New in version 0.22.\n",
    "\n",
    "**max_samples**int or float, default=None\n",
    "\n",
    "If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "\n",
    "-   If None (default), then draw  `X.shape[0]`  samples.\n",
    "    \n",
    "-   If int, then draw  `max_samples`  samples.\n",
    "    \n",
    "-   If float, then draw  `max_samples  *  X.shape[0]`  samples. Thus,  `max_samples`  should be in the interval  `(0.0,  1.0]`.\n",
    "    \n",
    "\n",
    "New in version 0.22.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "module includes Support Vector Machine algorithms.\n",
    "\n",
    "### Estimators\n",
    "\n",
    "[`svm.LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC \"sklearn.svm.LinearSVC\")([penalty, loss, dual, tol, C, ...])\n",
    "\n",
    "Linear Support Vector Classification.\n",
    "\n",
    "[`svm.LinearSVR`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR \"sklearn.svm.LinearSVR\")(*[, epsilon, tol, C, loss, ...])\n",
    "\n",
    "Linear Support Vector Regression.\n",
    "\n",
    "[`svm.NuSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC \"sklearn.svm.NuSVC\")(*[, nu, kernel, degree, gamma, ...])\n",
    "\n",
    "Nu-Support Vector Classification.\n",
    "\n",
    "[`svm.NuSVR`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR \"sklearn.svm.NuSVR\")(*[, nu, C, kernel, degree, gamma, ...])\n",
    "\n",
    "Nu Support Vector Regression.\n",
    "\n",
    "[`svm.OneClassSVM`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM \"sklearn.svm.OneClassSVM\")(*[, kernel, degree, gamma, ...])\n",
    "\n",
    "Unsupervised Outlier Detection.\n",
    "\n",
    "[`svm.SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC \"sklearn.svm.SVC\")(*[, C, kernel, degree, gamma, ...])\n",
    "\n",
    "C-Support Vector Classification.\n",
    "\n",
    "[`svm.SVR`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR \"sklearn.svm.SVR\")(*[, kernel, degree, gamma, coef0, ...])\n",
    "\n",
    "Epsilon-Support Vector Regression.\n",
    "\n",
    "[`svm.l1_min_c`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c \"sklearn.svm.l1_min_c\")(X, y, *[, loss, fit_intercept, ...])\n",
    "\n",
    "Return the lowest bound for C such that for C in (l1_min_C, infinity) the model is guaranteed not to be empty."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddbee94c2ce3b61c3b1774ea67d55f2fdae8c29319837f6f09ba2209f9564fc6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
